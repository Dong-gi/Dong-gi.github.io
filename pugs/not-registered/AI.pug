include ../../source/skeleton.pug

+post({
    title: 'AI',
    description: 'AI; 인공지능',
    useMath: true,
})

    h1 퍼셉트론; Perceptron
    ul
        li 입력 노드들의 입력과 가중치를 곱한 것을 모두 합한 값이 임계값보다 크면 뉴런이 활성화, 크지 않으면 비활성화
        li 단층 퍼셉트론은 공간을 선형적으로 잘라 이분법으로 분류한다
        p 예. 입력이 2개인 AND 게이트 
            +codeBtn('/Repositories/Python/ml/code_1.py', 'python')
        li 다층 퍼셉트론 : 다른 퍼셉트론의 출력을 입력으로 이용하는 퍼셉트론
        p 예. 입력이 2개인 NAND 게이트
            +codeBtn('/Repositories/Python/ml/code_2.py', 'python')

    h1 신경망; Neural network
    div
        h2 뉴런; Neuron
        ul
            li 신경망을 구성하는 노드로, 입출력 계층을 형성한다. 이때, raw 입력을 받는 계층을 입력층, 최종 출력하는 계층을 출력층, 나머지 중간을 은닉층이라고 부른다.
            li 입력층 외의 뉴런들은 각 입력에 가중치가 존재한다
            li 뉴런은 입출력 사이에 2개의 함수를 거친다
            ol
                li 식[\Sigma ::= x_{1}w_{1} + x_{2}w_{2} + \cdots + x_{n}w_{n} + b = a]식
                p 입력(x) * 가중치(w) 쌍들과 편향(b; bias)의 합
                li Activation function : 식[y = h(a)]식
                p 예를 들어, 퍼셉트론은 activation function으로 step function을 이용하는 뉴런이다
                    +codeBtn('/Repositories/Python/ml/code_3.py', 'python')

        h2 Sigmoid function
        ul
            li 단조 증가하는 S자 곡선을 갖는 함수로, 인공 신경망 뉴런의 activatino function으로 이용된다
            li Logistic function; 식[f(x) = {1 \over 1 + e^{-x}}]식
                +codeBtn('/Repositories/Python/ml/code_4.py', 'python')
            li Hyperbolic tangent; 식[f(x) = \operatorname{tanh}x = {e^x - e^{-x} \over e^x + e^{-x}}]식
            li Arctangent function; 식[f(x) = \operatorname{arctan}x]식
            li Error function; 식[f(x) = \operatorname{erf}(x) = {2 \over \sqrt{\pi}} \int_{0}^{x} e^{-t^2} dt]식
            li Generalised logistic function; 식[f(x) = (1 + e^{-x})^{-\alpha}, \alpha \gt 0]식
            li 식[f(x) = {x \over (1 + |x|^k)^{1/k}}]식

        h2 ReLU(Rectified Linear Unit) function
        p 0을 넘는 입력을 그대로 출력하는 함수로, 인공 신경망 뉴런의 activatino function으로 이용된다
        p 식[h(a) = \left\{\begin{matrix} a & (a \gt 0) \\ 0 & (a \le 0) \end{matrix}\right.]식 
            +codeBtn('/Repositories/Python/ml/code_5.py', 'python')

        h2 신경망 연산은 행렬의 내적으로 수행할 수 있다
        ul
            li i : i번째 계층(0부터 시작)
            li 식[w^{i}_{nm}]식 : i-1계층 m번 뉴런 -> i계층 n번 뉴런 가중치
            li 식[b^{i}_{n}]식 : i계층 n번 bias

    h1 scikit-learn
    ol
        li n_samples : 표본 개수
        li n_features : 변수 개수
        li n_classes : 분류할 클래스(출력) 개수
        li n_informative : n_features 중, 출력과 상관 관계가 있는 변수의 개수
        li n_redundant : n_features 중, 다른 변수의 선형 결합으로 나타내지는 변수의 개수
        li n_repeated : n_features 중, 완전히 동일한 변수의 개수
        li n_clusters_per_class : 클래스 당 클러스터 개수
        li random_state : 난수 시드
        li fit(self, X, y, sample_weight=None)
        ul
            li X : 학습 데이터. (n_samples, n_features) 크기의 유사 배열 또는 행렬 객체
            li y : 출력 클래스. (n_samples,) 또는 (n_samples, n_outputs) 크기의 유사 배열 객체

    h1 Classification
    div
        h2 Decesion Tree
        ol
            li leaf가 아닌 노드들은 데이터를 분류하기 위한 질문을 가진다
            span.hover-content(template-id='template-ai-impurity') 불순도
            |가 가장 작은 질문이 선택된다
            li leaf 노드는 클래스(회귀 모델에서는 확률값)를 가진다
            li 데이터는 루트 노드로부터 진입하여 질문에 따라 자식 노드로 이동한다
            li 정확성을 높이는 방안
            ul
                li 각 leaf 노드에 최소한 x개의 데이터가 분류되도록 제한
                li 트리의 최소 높이와 최대 높이를 제한
            li Python : from sklearn.tree import DecisionTreeClassifier

        h2 Random Forest
        ul
            li 서로 다른 학습 데이터로 만든 결정 트리들의 평균값으로 분류
            li Python : from sklearn.tree import RandomForestClassifier
            ul
                li n_estimators : 사용할 결정 트리의 개수. 많을수록 좋지만 사용 메모리 및 훈련 시간 증대
                li max_features : 각 트리당 사용할 변수의 개수. n_features에 가까울수록 각 결정 트리들이 서로 비슷해진다
                +table()
                    tr
                        td int
                        td max_features개 변수 사용
                    tr
                        td float
                        td int(max_features * n_features)개 변수 사용
                    tr
                        td "auto", "sqrt"
                        td sqrt(n_features)개 변수 사용
                    tr
                        td "log2"
                        td log2(n_features)개 변수 사용
                    tr
                        td None
                        td n_features개 변수 사용
        h2 SVM; Support Vector Machine
        ul
            li 선형 이진 분류기
            li 모든 벡터(데이터)의 차원이 동일한 경우, 적절한 원점을 설정해 만든 n차원 공간에서 
                span.hover-content(template-id='template-ai-margin') margin
                |이 최대인 
                span.hover-content(template-id='template-ai-hyperplane') 초평면
                |을 찾는다
            li 초평면의 법선 식[w]식, 초평면과 원점 사이의 거리 식[b]식, 임의 데이터 식[x]식에 대하여 식[f(x)=w \cdot x + b]식의 부호가 클래스를 결정한다
            li n차원 공간 [식[x_1, \cdots, x_n]식]에서는 초평면을 찾기 어렵지만, 적절히 차원을 늘리면 [식[x_1, \cdots, x_n, x_{n+1}, \cdots]식] 초평면을 찾기 쉬워질 수 있다
            ul
                li 식[x_{n+1} = k(x_1, \cdots, x_n)]식
                li 커널 함수 : 식[k(x, y) = k(y, x)]식
                li 다항 커널 : 식[k(x, y) = (x \cdot y + c)^n]식
                li 가우스 커널 : 식[k(x, y) = \operatorname{exp}(-\gamma |x-y|^2)]식
                li 시그모이드 커널 : 식[k(x, y) = \operatorname{tanh}(x \cdot y + r)]식

    +hoverTemplate()#template-ai-impurity
        |불순도 Impurity : 분류된 범주에 다른 범주의 데이터가 섞여있는 정도. 없으면 최소
        br
        |엔트로피 Entropy : 불순도를 수치화한 것. 식[-\sum_{k}P_k \log_{2}P_k]식. 식[P_k]식는 클래스 k로 분류된 데이터 중 실제로 클래스가 k인 데이터의 비율
    +hoverTemplate()#template-ai-margin
        |margin : 초평면과 가장 가까운 데이터 사이의 거리
    +hoverTemplate()#template-ai-hyperplane
        |초평면(超平面, 영어: hyperplane)은 3차원 공간 속의 평면을 일반화하여 얻는 개념이다
        +asA('https://ko.wikipedia.org/wiki/%EC%B4%88%ED%8F%89%EB%A9%B4_(%EC%88%98%ED%95%99)', '위키피디아')
