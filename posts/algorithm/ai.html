<html lnag="ko"><meta charset="UTF-8"><meta name="viewport"content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name="google-site-verification"content="aH-DyzdytYQ0NPHNQFcs5dVoiu5YNs6CEEHYgqDmAFM"><meta name="author"content="Donggi Kim &lt;hi.donggi@gmail.com&gt;"><meta name="keywords"content="AI, scikit-learn, Classification"><meta name="description"content="인공지능(AI) 공부하다 그만둔 것. 저는 프로그래밍이나 열심히 할게요. 수학과 파이팅!"><title>AI 공부하다 만 것</title><link id="w3css"rel="stylesheet"href="/source/w3.css"><link id="highlight-style"rel="stylesheet"href="/source/xcode.css"><link id="default-style"rel="stylesheet"href="/source/default.css"><script id="highlight-js"src="/source/highlight.pack.js"></script><script id="default-js"src="/source/default.min.js"></script><script id="dsq-count-scr"src="//donggi.disqus.com/count.js"async></script><link id="mjx-style"rel="stylesheet"href="/source/mjx-chtml.css"><script id="MathJax-script"src="/source/tex-chtml.js"async></script><script>MathJax={tex:{tags:"ams",inlineMath:[["식[","]식"]]}}</script><script id="google-analytics"src="https://www.googletagmanager.com/gtag/js?id=UA-143098403-1"async></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-143098403-1")</script><div id="sidebar"class="w3-bar-block w3-sidebar w3-animate-left"><hr><a href="javascript:closeSidebar();"class="w3-bar-item w3-button">Close &times;</a><hr><div id="marker-list"></div><hr><div id="post-list"><details open=""id="dir-1627423780"class="w3-small file-list"title="카테고리"><summary>카테고리</summary><ul><details id="dir--1093871206"class="w3-small file-list"title="카테고리/.NET"open=""><summary>/.NET</summary><ul><li><a href="/posts/dotnet/csharp_basic.html">C# 기초</a><li><a href="/posts/dotnet/csharp_library.html">C# 라이브러리</a><li><a href="/posts/dotnet/wpf_basic.html">WPF 기초</a></ul></details><details id="dir-1275489025"class="w3-small file-list"title="카테고리/단일 주제"open=""><summary>/단일 주제</summary><ul><li><a href="/posts/single/sdkman.html">SDKMAN; The Software Development Kit Manager</a></ul></details><details id="dir-992513986"class="w3-small file-list"title="카테고리/독서"open=""><summary>/독서</summary><ul><li><a href="/posts/book/001.html">1만 시간의 재발견</a><li><a href="/posts/book/002.html">마음의 탄생</a><li><a href="/posts/book/003.html">생각하는 삶을 위한 철학의 역사</a><li><a href="/posts/book/004.html">쇼펜하우어, 돌이 별이 되는 철학</a><li><a href="/posts/book/005.html">인생의 모든 의미</a><li><a href="/posts/book/009.html">인지 편향</a><li><a href="/posts/book/006.html">지식의 착각</a><li><a href="/posts/book/007.html">키르케고르 실존 극장</a><li><a href="/posts/book/008.html">Gamification by Design</a></ul></details><details id="dir-453420171"class="w3-small file-list"title="카테고리/알고리즘"open=""><summary>/알고리즘</summary><ul><li><a href="/posts/algorithm/book01.html">『알고리즘 도감』</a><li><a href="/posts/algorithm/linear_algebra.html">선형대수</a><li><a href="/posts/algorithm/algo.html">알고리즘 일반</a><li><a href="/posts/algorithm/probability.html">확률</a></li><details id="dir--1451989604"class="w3-small file-list"title="카테고리/알고리즘/KOREATECH"open=""><summary>/KOREATECH</summary><ul><li><a href="/posts/algorithm/koreatech/1003.html">1003: 0을 만들자</a><li><a href="/posts/algorithm/koreatech/1008.html">1008: 순환 소수</a><li><a href="/posts/algorithm/koreatech/1018.html">1018: 문자열 거리 최소화 하기</a><li><a href="/posts/algorithm/koreatech/1048.html">1048: AP 배분</a><li><a href="/posts/algorithm/koreatech/1095.html">1095: 자연스러운 정렬</a><li><a href="/posts/algorithm/koreatech/1034.html">1034,1041: 최소 이동 거리</a><li><a href="/posts/algorithm/koreatech/1011.html">동적계획법(DP) 문제</a><li><a href="/posts/algorithm/koreatech/1010.html">소수(Prime) 관련 문제</a></ul></details></ul></details><details id="dir-1817032068"class="w3-small file-list"title="카테고리/작성 중지"open=""><summary>/작성 중지</summary><ul><li><a href="/posts/algorithm/ai.html">AI</a></ul></details></ul></details></div><hr><div id="file-list"></div><hr></div><div id="main"><div id="nav"style="position:-webkit-sticky;position:sticky;top:0;vertical-align:middle"class="w3-bar w3-blue w3-large"><button onclick="toggleSidebar()"class="w3-bar-item w3-button w3-hover-theme">&#9776;</button><input id="query"placeholder="search"class="w3-bar-item"></div><div id="contents"class="w3-padding"><h1>AI 학습 정리 문서(미완, 수정 예정 없음)</h1><h2 class="marker">참고 자료</h2><table class="no-sort"><tr><td><td><a class="w3-btn w3-round w3-round-xxlarge w3-small w3-green"href="https://bkshin.tistory.com/category/머신러닝"target="_blank">https://bkshin.tistory.com/category/머신러닝</a><tr><td><td><a class="w3-btn w3-round w3-round-xxlarge w3-small w3-green"href="https://datascienceschool.net/view-notebook/"target="_blank">https://datascienceschool.net/view-notebook/</a><tr><td><td><a class="w3-btn w3-round w3-round-xxlarge w3-small w3-green"href="https://developers.google.com/machine-learning/crash-course/ml-intro?hl=ko"target="_blank">https://developers.google.com/machine-learning/crash-course/ml-intro?hl=ko</a><tr><td><td><a class="w3-btn w3-round w3-round-xxlarge w3-small w3-green"href="http://hunkim.github.io/ml/"target="_blank">http://hunkim.github.io/ml/</a><tr><td><td><a class="w3-btn w3-round w3-round-xxlarge w3-small w3-green"href="https://github.com/golbin/TensorFlow-Tutorials"target="_blank">https://github.com/golbin/TensorFlow-Tutorials</a><tr><td><td><a class="w3-btn w3-round w3-round-xxlarge w3-small w3-green"href="https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/"target="_blank">https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/</a><tr><td><td><a class="w3-btn w3-round w3-round-xxlarge w3-small w3-green"href="https://scikit-learn.org/stable/tutorial/"target="_blank">https://scikit-learn.org/stable/tutorial/</a><tr><td><td><a class="w3-btn w3-round w3-round-xxlarge w3-small w3-green"href="https://www.udacity.com/course/intro-to-tensorflow-for-deep-learning--ud187"target="_blank">https://www.udacity.com/course/intro-to-tensorflow-for-deep-learning--ud187</a></table><h2 class="marker">scikit-learn</h2><ol><li>n_samples : 표본 개수<li>n_features : 변수 개수<li>n_classes : 분류할 클래스(출력) 개수<li>n_informative : n_features 중, 출력과 상관 관계가 있는 변수의 개수<li>n_redundant : n_features 중, 다른 변수의 선형 결합으로 나타내지는 변수의 개수<li>n_repeated : n_features 중, 완전히 동일한 변수의 개수<li>n_clusters_per_class : 클래스 당 클러스터 개수<li>random_state : 난수 시드<li>fit(self, X, y, sample_weight=None)</li><ul><li>X : 학습 데이터. (n_samples, n_features) 크기의 유사 배열 또는 행렬 객체<li>y : 출력 클래스. (n_samples,) 또는 (n_samples, n_outputs) 크기의 유사 배열 객체</ul></ol><h2 class="marker">Classification</h2><h3 class="marker">Decesion Tree</h3><ol><li>leaf가 아닌 노드들은 데이터를 분류하기 위한 질문을 가진다</li><span template-id="template-ai-impurity"class="hover-content">불순도</span>가 가장 작은 질문이 선택된다<li>leaf 노드는 클래스(회귀 모델에서는 확률값)를 가진다<li>데이터는 루트 노드로부터 진입하여 질문에 따라 자식 노드로 이동한다<li>정확성을 높이는 방안</li><ul><li>각 leaf 노드에 최소한 x개의 데이터가 분류되도록 제한<li>트리의 최소 높이와 최대 높이를 제한</ul><li>Python : from sklearn.tree import DecisionTreeClassifier</ol><h3 class="marker">Random Forest</h3><ul><li>서로 다른 학습 데이터로 만든 결정 트리들의 평균값으로 분류<li>Python : from sklearn.tree import RandomForestClassifier</li><ul><li>n_estimators : 사용할 결정 트리의 개수. 많을수록 좋지만 사용 메모리 및 훈련 시간 증대<li>max_features : 각 트리당 사용할 변수의 개수. n_features에 가까울수록 각 결정 트리들이 서로 비슷해진다</li><table class="no-sort"><tr><td>int<td>max_features개 변수 사용<tr><td>float<td>int(max_features * n_features)개 변수 사용<tr><td>"auto", "sqrt"<td>sqrt(n_features)개 변수 사용<tr><td>"log2"<td>log2(n_features)개 변수 사용<tr><td>None<td>n_features개 변수 사용</table></ul></ul><h3 class="marker">SVM; Support Vector Machine</h3><ul><li>선형 이진 분류기<li>모든 벡터(데이터)의 차원이 동일한 경우, 적절한 원점을 설정해 만든 n차원 공간에서 <span template-id="template-ai-margin"class="hover-content">margin</span>이 최대인 <span template-id="template-ai-hyperplane"class="hover-content">초평면</span>을 찾는다<li>초평면의 법선 식[w]식, 초평면과 원점 사이의 거리 식[b]식, 임의 데이터 식[x]식에 대하여 식[f(x)=w \cdot x + b]식의 부호가 클래스를 결정한다<li>n차원 공간 [식[x_1, \cdots, x_n]식]에서는 초평면을 찾기 어렵지만, 적절히 차원을 늘리면 [식[x_1, \cdots, x_n, x_{n+1}, \cdots]식] 초평면을 찾기 쉬워질 수 있다</li><ul><li>식[x_{n+1} = k(x_1, \cdots, x_n)]식<li>커널 함수 : 식[k(x, y) = k(y, x)]식<li>다항 커널 : 식[k(x, y) = (x \cdot y + c)^n]식<li>가우스 커널 : 식[k(x, y) = \operatorname{exp}(-\gamma |x-y|^2)]식<li>시그모이드 커널 : 식[k(x, y) = \operatorname{tanh}(x \cdot y + r)]식</ul></ul><div class="w3-container w3-leftbar w3-border-green w3-white"id="template-ai-impurity">불순도 Impurity : 분류된 범주에 다른 범주의 데이터가 섞여있는 정도. 없으면 최소<br>엔트로피 Entropy : 불순도를 수치화한 것. 식[-\sum_{k}P_k \log_{2}P_k]식. 식[P_k]식는 클래스 k로 분류된 데이터 중 실제로 클래스가 k인 데이터의 비율.</div><div class="w3-container w3-leftbar w3-border-green w3-white"id="template-ai-margin">margin : 초평면과 가장 가까운 데이터 사이의 거리</div><div class="w3-container w3-leftbar w3-border-green w3-white"id="template-ai-hyperplane">초평면(超平面, 영어: hyperplane)은 3차원 공간 속의 평면을 일반화하여 얻는 개념이다.<a class="w3-btn w3-round w3-round-xxlarge w3-small w3-green"target="_blank"href="https://ko.wikipedia.org/wiki/%EC%B4%88%ED%8F%89%EB%A9%B4_(%EC%88%98%ED%95%99)">위키피디아</a></div></div><hr>&copy; 2020. <a href="mailto:hi.donggi@gmail.com">Donggi Kim</a> all rights reserved.<a class="w3-btn w3-round w3-round-xxlarge w3-small w3-green"href="/LICENSE"target="_blank">/LICENSE</a></div>